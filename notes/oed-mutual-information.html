<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Constant-time mutual information for Bayesian experimental design · Joseph S. Miller</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="A budgeted best-first adaptive partitioning estimator that performs Bayesian updating and mutual-information estimation together.">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:wght@300;400;500;600&display=swap" rel="stylesheet">
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
      },
      svg: {
        fontCache: 'global',
      },
    };
  </script>
  <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <style>
    :root {
      color-scheme: light;
      font-family: "Crimson Pro", "Times New Roman", serif;
      --ink: #1f2a37;
      --muted: #5b6673;
      --bg: #eef1f5;
      --panel: #ffffff;
      --accent: #8b3a3a;
      --accent-strong: #6d2b2b;
      --accent-soft: rgba(139, 58, 58, 0.14);
      --line: rgba(31, 42, 55, 0.12);
    }

    body {
      margin: 0;
      min-height: 100vh;
      background:
        linear-gradient(140deg, rgba(139, 58, 58, 0.05) 0%, transparent 55%),
        repeating-linear-gradient(0deg, rgba(139, 58, 58, 0.02), rgba(139, 58, 58, 0.02) 1px, transparent 1px, transparent 6px),
        var(--bg);
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 2.5rem 1.5rem;
      color: var(--ink);
    }

    .shell {
      width: min(1000px, 100%);
      background: #ffffff;
      border-radius: 26px;
      padding: 3rem;
      border: 1px solid rgba(31, 42, 55, 0.08);
      box-shadow: 0 22px 50px rgba(31, 42, 55, 0.12);
      position: relative;
      overflow: hidden;
    }

    .top-nav {
      display: flex;
      justify-content: space-between;
      align-items: center;
      gap: 1rem;
      padding-bottom: 1.25rem;
      margin-bottom: 2rem;
      border-bottom: 1px solid rgba(31, 42, 55, 0.08);
    }

    .nav-right {
      display: flex;
      gap: 1rem;
      flex-wrap: wrap;
      justify-content: flex-end;
      align-items: center;
    }

    .hero {
      display: grid;
      gap: 1.25rem;
    }

    .eyebrow {
      text-transform: uppercase;
      letter-spacing: 0.28em;
      font-size: 0.7rem;
      color: var(--accent-strong);
      font-weight: 600;
    }

    h1 {
      margin: 0;
      font-size: clamp(2.25rem, 4.2vw, 3.3rem);
      font-weight: 600;
      letter-spacing: 0.01em;
    }

    p {
      margin: 0;
      line-height: 1.7;
    }

    .lede {
      color: var(--muted);
      font-size: 1.1rem;
      max-width: 78ch;
    }

    .actions {
      display: flex;
      flex-wrap: wrap;
      gap: 0.75rem;
      align-items: center;
      margin-top: 0.25rem;
    }

    .button {
      background: var(--accent);
      color: #fff;
      padding: 0.65rem 1.3rem;
      border-radius: 999px;
      text-decoration: none;
      font-weight: 600;
      box-shadow: 0 10px 18px rgba(31, 42, 55, 0.16);
    }

    .button:hover,
    .button:focus-visible {
      background: var(--accent-strong);
      box-shadow: 0 14px 24px rgba(31, 42, 55, 0.2);
    }

    a {
      color: var(--accent-strong);
      text-decoration: none;
      font-weight: 600;
    }

    a:hover,
    a:focus-visible {
      color: var(--accent);
    }

    .text-link {
      color: var(--muted);
      font-weight: 500;
    }

    .text-link:hover,
    .text-link:focus-visible {
      color: var(--accent-strong);
    }

    .stack {
      margin-top: 2.75rem;
      display: grid;
      gap: 1.5rem;
    }

    .stack-tight {
      margin-top: 1.5rem;
      display: grid;
      gap: 1.5rem;
    }

    .grid {
      display: grid;
      grid-template-columns: repeat(2, minmax(0, 1fr));
      gap: 1.5rem;
    }

    .panel {
      background: var(--panel);
      border: 1px solid var(--line);
      border-radius: 18px;
      padding: 1.75rem;
      box-shadow: 0 12px 24px rgba(31, 42, 55, 0.08);
      position: relative;
    }

    .panel::before {
      content: "";
      position: absolute;
      left: 1.75rem;
      top: 0.85rem;
      width: 42px;
      height: 2px;
      border-radius: 999px;
      background: var(--accent);
    }

    .panel h2 {
      margin: 0 0 1rem;
      font-size: 0.92rem;
      letter-spacing: 0.2em;
      text-transform: uppercase;
      color: var(--accent-strong);
      font-weight: 600;
    }

    .panel p + p {
      margin-top: 0.9rem;
    }

    .panel p + .bullets {
      margin-top: 0.9rem;
    }

    .panel .bullets + p {
      margin-top: 0.9rem;
    }

    .bullets {
      margin: 0;
      padding-left: 1.25rem;
      line-height: 1.7;
    }

    .bullets li {
      margin: 0.35rem 0;
    }

    .math-block {
      margin: 0.5rem 0 0;
      overflow-x: auto;
    }

    figure {
      margin: 0;
      display: grid;
      gap: 0.85rem;
    }

    figure img {
      width: 100%;
      height: auto;
      border-radius: 14px;
      border: 1px solid rgba(31, 42, 55, 0.08);
      background: #ffffff;
    }

    figcaption {
      color: var(--muted);
      line-height: 1.6;
      font-size: 1rem;
    }

    figcaption strong {
      color: var(--ink);
      font-weight: 600;
    }

    details {
      padding-top: 0.9rem;
      border-top: 1px solid rgba(31, 42, 55, 0.08);
    }

    details:first-of-type {
      padding-top: 0;
      border-top: none;
    }

    summary {
      cursor: pointer;
      font-weight: 600;
      color: var(--ink);
      list-style: none;
    }

    summary::-webkit-details-marker {
      display: none;
    }

    details p {
      margin-top: 0.65rem;
      color: var(--muted);
    }

    .callout {
      background: rgba(139, 58, 58, 0.04);
      border-color: rgba(139, 58, 58, 0.18);
    }

    .callout::before {
      background: var(--accent-strong);
    }

    .muted {
      color: var(--muted);
    }

    @media (max-width: 900px) {
      .grid {
        grid-template-columns: 1fr;
      }
    }

    @media (max-width: 700px) {
      .shell {
        padding: 2.25rem 1.75rem;
      }
    }
  </style>
</head>
<body>
  <div class="shell">
    <nav class="top-nav">
      <a class="text-link" href="../index.html">← Home</a>
      <div class="nav-right">
        <a class="text-link" href="index.html">Notes</a>
        <a class="text-link" href="mailto:joe@josephsmiller.com">Email</a>
      </div>
    </nav>

    <header class="hero">
      <div class="eyebrow">Technical note</div>
      <h1>Constant-time mutual information for Bayesian experimental design</h1>
      <p class="lede">
        A budgeted, best-first adaptive partitioning estimator that performs Bayesian updating and mutual-information
        estimation together—delivering stable, tunable, constant-time MI for differentiable models without requiring
        separate posterior sampling.
      </p>
      <div class="actions">
        <a class="button" href="mailto:joe@josephsmiller.com?subject=Optimal%20experiment%20design%20MI">Discuss a project</a>
        <a class="text-link" href="#results">Jump to results</a>
      </div>
    </header>

    <main class="stack">
      <section class="panel">
        <h2>Motivation</h2>
        <p>
          A common applied workflow is: collect data, train a model, then deploy it. Optimal experiment design interrupts
          that process. While the model is still being trained, you use what it currently knows to select the next data to
          collect—so the model learns faster, reaches useful performance sooner, and you spend fewer resources on
          uninformative observations.
        </p>
        <p class="muted">
          This is especially valuable when the pool of possible observations is much larger than what will ever be used
          for training (clinical trials, A/B testing, ad experiments, surveys): it can be worth spending real compute to
          choose informative data rather than selecting blindly, because the opportunity cost of collecting the “wrong”
          data (and delaying decisions) is often larger than the cost of the selection algorithm.
        </p>
        <p>
          In Bayesian optimal experiment design, this idea is formalized by scoring candidate designs by <em>expected
          information gain</em>. A common choice is mutual information (MI): the “best” experiment is the one expected to
          reduce uncertainty the most about whatever you care about (parameters, predictions, or another derived random
          variable).
        </p>
        <p class="muted">
          Mutual information can be written as an expected reduction in entropy. For a proposed batch design
          $\mathcal{D}_{1:k}$ and outcomes $y_{1:k}$, the parameter-learning objective is:
        </p>
        <div class="math-block">
          $$I(\theta; y_{1:k} \mid \mathcal{D}_{1:k})
          = H\!\left[p(\theta)\right]
          - \mathrm{E}_{y_{1:k} \sim p(\cdot \mid \mathcal{D}_{1:k})}
            \left[H\!\left[p(\theta \mid \mathcal{D}_{1:k}, y_{1:k})\right]\right].$$
        </div>
        <p class="muted">
          Here $\theta$ denotes the current uncertain state of the model at the moment you score designs (e.g., a current
          posterior state). Intuitively: start from your current uncertainty, then subtract the expected uncertainty
          after running the batch and updating the model.
        </p>
        <p class="muted">
          Predictive MI uses the same form, replacing $\theta$ with the predictive quantity of interest.
        </p>
        <p class="muted">
          Small modifications are useful when the relevant notion of “uncertainty” isn’t purely Shannon entropy—for
          example, using similarity-sensitive entropy to encode which distinctions matter for your downstream goal. See
          <a href="https://arxiv.org/abs/2601.03064" target="_blank" rel="noreferrer">Similarity-Sensitive Entropy:
          Induced Kernels and Data-Processing Inequalities (arXiv:2601.03064)</a> for a framework that helps clarify
          whether the uncertainty you care about is parameter uncertainty, predictive uncertainty, or uncertainty in some
          other derived random variable.
        </p>
        <p>
          In principle, you can run this continuously: score designs in real time, collect the next batch, update the
          model, then re-score after every update.
        </p>
        <p class="muted">In practice, routinely computing MI inside a live data pipeline is hard:</p>
        <ul class="bullets">
          <li>
            <strong>Two coupled uncertainty problems:</strong> you need the current model state (which changes as data
            arrives) and you must average over uncertain future outcomes $y_{1:k}$.
          </li>
          <li><strong>Expensive estimators:</strong> enumeration scales as $2^k$, while nested Monte Carlo can be variance-limited without large inner loops.</li>
          <li><strong>Inference dependency:</strong> many estimators assume posterior samples exist (HMC/MCMC) and must be managed and refreshed.</li>
          <li><strong>Engineering friction:</strong> implementing, tuning, and serving MI under latency constraints doesn’t fit many teams.</li>
        </ul>
        <p class="muted">
          Best-first adaptive partitioning addresses these bottlenecks by making MI evaluation budgeted and predictable,
          and by performing Bayesian updating as part of MI estimation—so inference and information gain come as one
          package that can run in production even for small teams and small businesses without dedicated Bayesian
          infrastructure.
        </p>
      </section>

      <section class="panel" id="results">
        <h2>Results</h2>
        <p class="muted">
          These figures compare multiple estimators of mutual information (MI) used for Bayesian experimental design,
          across two representative model families (a linear-Gaussian baseline and a nonconjugate ridge-logistic model).
          The headline: best-first adaptive partitioning delivers an anytime, tunable, constant-time-in-(k) MI estimator
          that does not require posterior samples, while remaining competitive in fidelity with strong sampling-based
          baselines.
        </p>
        <p class="muted">
          Terminology: “Oracle” refers to the high-quality reference used to score fidelity (correlation). In addition,
          the figures include a couple of problem-specific ceiling baselines (“MC analytic” in the linear-Gaussian case
          and “Exhaustive importance” in the ridge-logistic case). These are not general-purpose estimators—many models
          don’t admit shortcuts like these—and they are included to show what’s achievable when you’re willing to write
          custom, model-tailored machinery.
        </p>

        <div class="stack-tight">
          <figure>
            <img
              src="assets/mi_all_summary_linear.png"
              alt="Linear-Gaussian results: correlation and runtime versus batch size for posterior MI and predictive MI."
              loading="lazy"
              decoding="async"
            >
            <figcaption>
              <strong>Linear-Gaussian model.</strong> Top row: correlation with the Oracle reference vs batch size (k) for
              posterior MI (left) and predictive MI (right). Bottom row: average time per MI query vs (k). With a fixed
              refinement budget, best-first adaptive partitioning keeps runtime essentially flat as (k) grows while
              maintaining strong correlation.
            </figcaption>
          </figure>

          <figure>
            <img
              src="assets/mi_all_summary_ridge_logistic.png"
              alt="Ridge-logistic results: correlation and runtime versus batch size for posterior MI and predictive MI."
              loading="lazy"
              decoding="async"
            >
            <figcaption>
              <strong>Ridge-logistic model.</strong> The same story holds in a nonconjugate setting: competitive fidelity
              without posterior samples, and predictable compute thanks to explicit global refinement budgets.
            </figcaption>
          </figure>

          <figure>
            <img
              src="assets/mi_spread_summary.png"
              alt="Spread summary: box and whisker plots summarizing variability across batch sizes for correlation and runtime."
              loading="lazy"
              decoding="async"
            >
            <figcaption>
              <strong>Across-(k) variability.</strong> Box/whisker summaries for correlation and time highlight stability
              and predictable compute—especially important when MI is queried repeatedly inside design loops.
            </figcaption>
          </figure>
        </div>
      </section>

      <div class="grid">
        <section class="panel">
          <h2>What’s measured</h2>
          <ul class="bullets">
            <li>
              For each model, we estimate MI for batches of experiments of size (k), averaged over many randomly chosen
              design subsets (apples-to-apples across methods at each (k)).
            </li>
            <li>
              Two objectives: posterior MI (learning parameters) and predictive MI (learning predictions at target
              inputs).
            </li>
            <li>
              Timing reports average time per MI evaluation (given whatever cached data the method uses), which is the
              relevant cost when selecting designs by evaluating MI many times.
            </li>
          </ul>
        </section>

        <section class="panel">
          <h2>Why it matters</h2>
          <ul class="bullets">
            <li><strong>Predictable compute:</strong> fixed budgets make runtime effectively constant in (k).</li>
            <li><strong>Anytime:</strong> usable early estimates that improve monotonically with more budget.</li>
            <li><strong>Global refinement:</strong> spends compute on high-impact outcome regions.</li>
            <li><strong>No posterior sampling:</strong> Bayesian updating happens inside the MI estimator.</li>
            <li><strong>Unified objectives:</strong> the same machinery supports posterior and predictive MI.</li>
          </ul>
        </section>
      </div>

      <section class="panel">
        <h2>How it works</h2>
        <p>
          Best-first adaptive partitioning is a tree-based adaptive quadrature over the discrete outcome space. It starts
          with a coarse partition, maintains an approximate posterior state per region, and repeatedly refines the
          regions with the highest estimated global impact—subject to a strict compute budget.
        </p>
        <p class="muted">
          Holding the budget fixed makes compute predictable as batch size grows, which is exactly what you want in
          experiment design loops that evaluate MI repeatedly across many candidate designs.
        </p>
      </section>

      <section class="panel">
        <h2>Fair comparisons</h2>
        <p>
          Sampling-based baselines typically require posterior samples (e.g., via HMC/MCMC). That inference can be a large
          one-time fixed-data cost, but it’s still a practical barrier in real workflows. Best-first adaptive partitioning
          avoids that dependency by updating posterior state as part of MI estimation.
        </p>
        <p class="muted">
          To avoid subtle “training on the test set” effects, each sampling-based method uses its own independent posterior
          draw stream (not shared across methods or with the Oracle reference).
        </p>
      </section>

      <section class="panel">
        <h2>Methods compared</h2>
        <details open>
          <summary>Best-first adaptive partitioning</summary>
          <p>
            Budgeted best-first refinement over outcome partitions, with approximate posterior updates attached to each
            region. Anytime, tunable compute, and no posterior samples required.
          </p>
        </details>
        <details>
          <summary>Nested Monte Carlo (NMC)</summary>
          <p>
            General-purpose MI estimator with nested sampling loops. Broadly applicable but can be expensive and
            variance-limited unless inner sample sizes grow.
          </p>
        </details>
        <details>
          <summary>Variational nested Monte Carlo (VNMC)</summary>
          <p>
            Uses variational approximations to tighten bounds and reduce variance relative to NMC, but remains
            sample-based and still relies on inference machinery.
          </p>
        </details>
        <details>
          <summary>Variational posterior bound (Laplace)</summary>
          <p>
            Fast approximation via a bound and local posterior geometry; performance depends on how well the approximation
            matches the true posterior (often weaker in nonconjugate regimes).
          </p>
        </details>
        <details>
          <summary>Importance sampling (y-sampled)</summary>
          <p>
            Avoids exhaustive outcome enumeration by sampling outcomes with a proposal distribution; can be brittle when
            outcomes are imbalanced or proposals mismatch the predictive distribution.
          </p>
        </details>
        <details>
          <summary>Problem-specific ceiling baselines (MC analytic / exhaustive importance)</summary>
          <p>
            These are not drop-in estimators you’d deploy in a generic design loop. They are custom, model-specific
            reference computations that either exploit analytic structure or use very expensive, problem-tailored
            importance schemes (often alongside HMC) to approximate MI as accurately as practical for these benchmarks.
          </p>
          <p>
            They’re included to ground the comparisons: rather than arguing against weak baselines, the figures show how
            close each scalable method gets to a credible ceiling when custom, model-tailored tools are available.
          </p>
        </details>
      </section>

      <section class="panel callout">
        <h2>Want this in your workflow?</h2>
        <p>
          If you’re selecting experiments with MI (or need a fast, stable way to score designs without maintaining an
          MCMC pipeline), I’m happy to talk through your setup and whether a budgeted estimator is a good fit.
        </p>
        <div class="actions">
          <a class="button" href="mailto:joe@josephsmiller.com?subject=Mutual%20information%20design%20inquiry">Email me</a>
          <a class="text-link" href="../index.html">Back to homepage</a>
        </div>
      </section>
    </main>
  </div>
</body>
</html>
