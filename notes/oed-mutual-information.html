<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Constant-time mutual information for Bayesian experimental design · Joseph S. Miller</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="A budgeted best-first adaptive partitioning estimator that performs Bayesian updating and mutual-information estimation together.">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:wght@300;400;500;600&display=swap" rel="stylesheet">
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
      },
      svg: {
        fontCache: 'global',
      },
    };
  </script>
  <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <style>
    :root {
      color-scheme: light;
      font-family: "Crimson Pro", "Times New Roman", serif;
      --ink: #1f2a37;
      --muted: #5b6673;
      --bg: #eef1f5;
      --panel: #ffffff;
      --accent: #8b3a3a;
      --accent-strong: #6d2b2b;
      --accent-soft: rgba(139, 58, 58, 0.14);
      --line: rgba(31, 42, 55, 0.12);
    }

    body {
      margin: 0;
      min-height: 100vh;
      background:
        linear-gradient(140deg, rgba(139, 58, 58, 0.05) 0%, transparent 55%),
        repeating-linear-gradient(0deg, rgba(139, 58, 58, 0.02), rgba(139, 58, 58, 0.02) 1px, transparent 1px, transparent 6px),
        var(--bg);
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 2.5rem 1.5rem;
      color: var(--ink);
    }

    .shell {
      width: min(1000px, 100%);
      background: #ffffff;
      border-radius: 26px;
      padding: 3rem;
      border: 1px solid rgba(31, 42, 55, 0.08);
      box-shadow: 0 22px 50px rgba(31, 42, 55, 0.12);
      position: relative;
      overflow: hidden;
    }

    .top-nav {
      display: flex;
      justify-content: space-between;
      align-items: center;
      gap: 1rem;
      padding-bottom: 1.25rem;
      margin-bottom: 2rem;
      border-bottom: 1px solid rgba(31, 42, 55, 0.08);
    }

    .nav-right {
      display: flex;
      gap: 1rem;
      flex-wrap: wrap;
      justify-content: flex-end;
      align-items: center;
    }

    .hero {
      display: grid;
      gap: 1.25rem;
    }

    .eyebrow {
      text-transform: uppercase;
      letter-spacing: 0.28em;
      font-size: 0.7rem;
      color: var(--accent-strong);
      font-weight: 600;
    }

    h1 {
      margin: 0;
      font-size: clamp(2.25rem, 4.2vw, 3.3rem);
      font-weight: 600;
      letter-spacing: 0.01em;
    }

    p {
      margin: 0;
      line-height: 1.7;
    }

    .lede {
      color: var(--muted);
      font-size: 1.1rem;
      max-width: 78ch;
    }

    .actions {
      display: flex;
      flex-wrap: wrap;
      gap: 0.75rem;
      align-items: center;
      margin-top: 0.25rem;
    }

    .button {
      background: var(--accent);
      color: #fff;
      padding: 0.65rem 1.3rem;
      border-radius: 999px;
      text-decoration: none;
      font-weight: 600;
      box-shadow: 0 10px 18px rgba(31, 42, 55, 0.16);
    }

    .button:hover,
    .button:focus-visible {
      background: var(--accent-strong);
      box-shadow: 0 14px 24px rgba(31, 42, 55, 0.2);
    }

    a {
      color: var(--accent-strong);
      text-decoration: none;
      font-weight: 600;
    }

    a:hover,
    a:focus-visible {
      color: var(--accent);
    }

    .text-link {
      color: var(--muted);
      font-weight: 500;
    }

    .text-link:hover,
    .text-link:focus-visible {
      color: var(--accent-strong);
    }

    .stack {
      margin-top: 2.75rem;
      display: grid;
      gap: 1.5rem;
      grid-template-columns: minmax(0, 1fr);
    }

    .stack-tight {
      margin-top: 1.5rem;
      display: grid;
      gap: 1.5rem;
      grid-template-columns: minmax(0, 1fr);
    }

    .grid {
      display: grid;
      grid-template-columns: repeat(2, minmax(0, 1fr));
      gap: 1.5rem;
    }

    .panel {
      background: var(--panel);
      border: 1px solid var(--line);
      border-radius: 18px;
      padding: 1.75rem;
      box-shadow: 0 12px 24px rgba(31, 42, 55, 0.08);
      position: relative;
      min-width: 0;
    }

    .panel::before {
      content: "";
      position: absolute;
      left: 1.75rem;
      top: 0.85rem;
      width: 42px;
      height: 2px;
      border-radius: 999px;
      background: var(--accent);
    }

    .panel h2 {
      margin: 0 0 1rem;
      font-size: 0.92rem;
      letter-spacing: 0.2em;
      text-transform: uppercase;
      color: var(--accent-strong);
      font-weight: 600;
    }

    .panel p + p {
      margin-top: 0.9rem;
    }

    .panel p + .bullets {
      margin-top: 0.9rem;
    }

    .panel .bullets + p {
      margin-top: 0.9rem;
    }

    pre {
      margin: 0.9rem 0;
      padding: 1rem 1.1rem;
      border-radius: 14px;
      border: 1px solid rgba(31, 42, 55, 0.12);
      background: rgba(31, 42, 55, 0.04);
      overflow-x: auto;
      max-width: 100%;
      box-sizing: border-box;
      -webkit-overflow-scrolling: touch;
    }

    code {
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", monospace;
      font-size: 0.95em;
    }

    .code-grid {
      display: grid;
      grid-template-columns: repeat(2, minmax(0, 1fr));
      gap: 1.25rem;
      margin-top: 1rem;
    }

    .code-grid.single {
      grid-template-columns: 1fr;
    }

    .code-grid pre {
      margin: 0;
    }

    .code-grid > * {
      min-width: 0;
    }

    .code-title {
      text-transform: uppercase;
      letter-spacing: 0.22em;
      font-size: 0.75rem;
      color: var(--accent-strong);
      font-weight: 600;
      margin: 0 0 0.55rem;
    }

    .bullets {
      margin: 0;
      padding-left: 1.25rem;
      line-height: 1.7;
    }

    .bullets li {
      margin: 0.35rem 0;
    }

    .math-block {
      margin: 0.5rem 0 0;
      overflow-x: auto;
      max-width: 100%;
      -webkit-overflow-scrolling: touch;
    }

    mjx-container[jax="SVG"][display="true"] {
      max-width: 100%;
      overflow-x: auto;
      overflow-y: hidden;
      -webkit-overflow-scrolling: touch;
    }

    figure {
      margin: 0;
      display: grid;
      gap: 0.85rem;
    }

    figure img {
      width: 100%;
      height: auto;
      border-radius: 14px;
      border: 1px solid rgba(31, 42, 55, 0.08);
      background: #ffffff;
    }

    figcaption {
      color: var(--muted);
      line-height: 1.6;
      font-size: 1rem;
    }

    figcaption strong {
      color: var(--ink);
      font-weight: 600;
    }

    details {
      padding-top: 0.9rem;
      border-top: 1px solid rgba(31, 42, 55, 0.08);
    }

    details:first-of-type {
      padding-top: 0;
      border-top: none;
    }

    summary {
      cursor: pointer;
      font-weight: 600;
      color: var(--ink);
      list-style: none;
    }

    summary::-webkit-details-marker {
      display: none;
    }

    details p {
      margin-top: 0.65rem;
      color: var(--muted);
    }

    .callout {
      background: rgba(139, 58, 58, 0.04);
      border-color: rgba(139, 58, 58, 0.18);
    }

    .callout::before {
      background: var(--accent-strong);
    }

    .muted {
      color: var(--muted);
    }

    @media (max-width: 900px) {
      .grid {
        grid-template-columns: 1fr;
      }

      .code-grid {
        grid-template-columns: 1fr;
      }
    }

    @media (max-width: 700px) {
      .shell {
        padding: 2.25rem 1.75rem;
      }
    }
  </style>
</head>
<body>
  <div class="shell">
    <nav class="top-nav">
      <a class="text-link" href="../index.html">← Home</a>
      <div class="nav-right">
        <a class="text-link" href="index.html">Notes</a>
        <a class="text-link" href="mailto:joe@josephsmiller.com">Email</a>
      </div>
    </nav>

    <header class="hero">
      <div class="eyebrow">Technical note</div>
      <h1>Constant-time mutual information for Bayesian experimental design</h1>
      <p class="lede">
        A budgeted, best-first adaptive partitioning estimator that performs Bayesian updating and mutual-information
        estimation together—delivering stable, tunable, constant-time MI for differentiable models without requiring
        separate posterior sampling or an MCMC pipeline.
      </p>
      <ul class="bullets muted">
        <li><strong>Predictable compute:</strong> with a fixed refinement budget, per-query runtime stays approximately flat as batch size $k$ grows.</li>
        <li><strong>Anytime estimates:</strong> usable early scores that improve monotonically as you spend more budget.</li>
        <li><strong>Simpler integration:</strong> no posterior samples or separate inference/scoring services to maintain.</li>
      </ul>
      <div class="actions">
        <a class="button" href="mailto:joe@josephsmiller.com?subject=Optimal%20experiment%20design%20MI">Discuss a project</a>
      </div>
    </header>

    <main class="stack">
      <section class="panel">
        <h2>Motivation</h2>
        <p>
            A common applied workflow is: collect data, train a model, then deploy it once it is accurate enough. In Bayesian
            inference, the model encodes your current state of knowledge (and therefore, uncertainty) given what you have observed so far.
            During data collection, optimal experiment design (OED) uses that current state to decide which experiment or
            observation to run next so that uncertainty shrinks as quickly as possible. Done well, this can reduce the amount
            of data you need to reach a useful level of performance, shorten time-to-decision, and avoid spending resources
            (or opportunity cost) on uninformative measurements.
        </p>
        <p class="muted">
          OED is especially valuable when the pool of possible observations is much larger than what will ever be used
          for training (clinical trials, A/B testing, ad experiments, surveys): it can be worth spending real compute to
          choose informative data rather than selecting blindly, because the opportunity cost of collecting the “wrong”
          data (and delaying decisions) is often larger than the cost of the selection algorithm.
        </p>
        <p>
          In typical variants of optimal experiment design, this idea is formalized by scoring candidate designs by <em>expected
          information gain</em>. A common choice is mutual information (MI): the “best” experiment is the one expected to
          reduce uncertainty the most about whatever you care about (parameters, predictions, or another derived random
          variable).
        </p>
        <p class="muted">
          Mutual information can be written as an expected reduction in entropy. For a proposed batch design
          $\mathcal{D}_{1:k}$ and outcomes $y_{1:k}$, the parameter-learning objective is:
        </p>
        <div class="math-block">
          $$I(\theta; y_{1:k} \mid \mathcal{D}_{1:k})
          = H\!\left[p(\theta)\right]
          - \mathrm{E}_{y_{1:k} \sim p(\cdot \mid \mathcal{D}_{1:k})}
            \left[H\!\left[p(\theta \mid \mathcal{D}_{1:k}, y_{1:k})\right]\right].$$
        </div>
        <p class="muted">
          Here $\theta$ denotes the current uncertain state of the model at the moment you score designs (e.g., a current
          posterior state). Intuitively: start from your current uncertainty, then subtract the expected uncertainty
          after running the batch and updating the model.
        </p>
        <p class="muted">
          Predictive MI uses the same form, replacing $\theta$ with the predictive quantity of interest.
        </p>
        <p class="muted">
          Small modifications are useful when the relevant notion of “uncertainty” isn’t purely Shannon entropy—for
          example, using similarity-sensitive entropy to encode which distinctions matter for your downstream goal. See
          <a href="https://arxiv.org/abs/2601.03064" target="_blank" rel="noreferrer">Similarity-Sensitive Entropy:
          Induced Kernels and Data-Processing Inequalities (arXiv:2601.03064)</a> for a framework that helps clarify
          whether the uncertainty you care about is parameter uncertainty, predictive uncertainty, or uncertainty in some
          other derived random variable.
        </p>
        <p class="muted">
          Here $k$ is the batch size. The design $\mathcal{D}_{1:k}$ represents a set of $k$ candidate experiments (design
          points, treatments, prompts, etc.), and $y_{1:k}$ is the corresponding batch of outcomes you will observe. Before
          data collection, they are unknown, but depend on your model’s predictive distribution and $\mathcal{D}_{1:k}$.
        </p>
        <p>
          When you can run multiple experiments, selecting the single best experiment (repeatedly choosing $k{=}1$) can be
          suboptimal: you want a batch that is informative <em>together</em>, avoiding redundancy and covering the space of
          uncertainty efficiently.
        </p>
        <p>
          In principle, you can run this continuously: score designs in real time, collect the next batch, update the
          model, then re-score after every update.
        </p>
        <p class="muted">In practice, routinely computing MI inside a live data pipeline is hard:</p>
        <ul class="bullets">
          <li>
            <strong>Batch combinatorics:</strong> if you have $n$ candidate experiments and you choose batches of size $k$,
            there are $\binom{n}{k}$ possible batches. Even with greedy or heuristic search, you end up calling the MI
            estimator many times.
          </li>
          <li>
            <strong>$k$-dimensional averaging:</strong> the expectation over $y_{1:k}$ is a $k$-dimensional sum/integral.
            Naive enumeration can scale as $2^k$ for binary outcomes, and straightforward quadrature becomes intractable as
            $k$ grows.
          </li>
          <li>
            <strong>Posterior refresh + state handoff:</strong> many estimators assume posterior samples exist (HMC/MCMC).
            As new data arrives, you pay an inference cost to refresh the model state, ship samples/state to the MI
            evaluator, score candidate batches, then repeat.
          </li>
          <li>
            <strong>Engineering friction:</strong> implementing, tuning, and serving MI under latency constraints doesn’t
            fit many teams.
          </li>
        </ul>      </section>

      <section class="panel">
        <h2>Best-first adaptive partitioning</h2>
        <p>
          Best-first adaptive partitioning addresses these bottlenecks directly. It's a tree-based adaptive quadrature over
          the discrete outcome space that performs Bayesian updating <em>inside</em> the MI computation—so you can score
          candidate batches without maintaining posterior samples or a separate inference service. By using an explicit
          global refinement budget, per-query compute stays approximately flat as batch size $k$ grows, making it practical
          to evaluate MI repeatedly in a live design loop.
        </p>        <p class="muted">
          “Constant-time in $k$” here means: with a fixed global refinement budget, runtime per MI query is approximately
          independent of $k$. Increasing the budget trades runtime for fidelity.
        </p>
        <p class="muted">
          In integration terms, you can treat it as a single scoring function: given your regression model specification,
          observed data, and a candidate batch $\mathcal{D}_{1:k}$, it returns an information-gain score for that batch.
          “Model specification” can be as simple as a likelihood (and, when available, gradients/Hessians for efficient
          local updates).
        </p>
        <pre><code>information_gain(model_spec, observed_data, candidate_batch) -&gt; score</code></pre>
        <p class="muted">
          <strong>Where this estimator shines:</strong> repeated MI evaluations inside a design loop (especially batch search), where you
          need a reliable per-query latency budget and don’t want to maintain posterior samples or a separate inference service, or you don't
          have an inference service at all yet (it comes for free). If you have a highly complex or non-Bayesian model but want to experiment
          with scoring designs, this can also be a low-overhead way to get started.
        </p>
        <p class="muted">
          <strong>Where it’s less compelling:</strong> settings where you already have high-quality posterior draws on hand, have highly complex
          models without analytic likelihoods, or are not bottlenecked by latency.
        </p>
        <p class="muted"><strong>Integration view:</strong> before (two-service pipeline) vs after (single scoring call).</p>
        <div class="code-grid">
          <div>
            <div class="code-title">Before · Inference Service</div>
            <pre><code>observed_data += new_observations
posterior_samples = run_inference(model_spec, observed_data)   # e.g., HMC/MCMC
artifact_uri = write_to_storage(posterior_samples)             # serialize + publish state</code></pre>
          </div>
          <div>
            <div class="code-title">Before · MI Scoring Service</div>
            <pre><code>posterior_samples = read_from_storage(artifact_uri)
score = mi_estimate(posterior_samples, candidate_batch)        # repeated many times during batch search</code></pre>
          </div>
        </div>
        <div class="code-grid single">
          <div>
            <div class="code-title">After · Single Scoring Call</div>
            <pre><code>observed_data += new_observations
score = information_gain(model_spec, observed_data, candidate_batch)</code></pre>
          </div>
        </div>
        <p class="muted">
          No posterior samples and no separate inference/scoring service to maintain—just raw data in and an interpretable
          score out. Dependencies can be minimal: standard numerical linear algebra plus your likelihood/model code.
        </p>
      </section>

      <section class="panel" id="results">
        <h2>Results</h2>
        <p class="muted">
          These figures compare multiple estimators of mutual information (MI) used for Bayesian experimental design,
          across two representative model families (a linear-Gaussian baseline and a nonconjugate ridge-logistic model).
          The headline: best-first adaptive partitioning delivers an anytime, tunable, constant-time-in-$k$ MI estimator
          that does not require posterior samples, while remaining competitive in fidelity with strong sampling-based
          baselines.
        </p>
        <p class="muted">
          For context: on these small benchmarks, refreshing the sampling-based baselines via HMC/MCMC takes about 0.1s
          (~100ms) per inference run.
        </p>
        <p class="muted">
          Terminology: “Oracle” refers to the high-quality reference used to score fidelity (correlation). In addition,
          the figures include a couple of problem-specific ceiling baselines (“MC analytic” in the linear-Gaussian case
          and “Exhaustive importance” in the ridge-logistic case). These are not general-purpose estimators—many models
          don’t admit shortcuts like these—and they are included to show what’s achievable when you’re willing to write
          custom, model-tailored machinery for these convenient model classes.
        </p>

        <div class="stack-tight">
          <figure>
            <img
              src="assets/mi_all_summary_linear.png"
              alt="Linear-Gaussian results: correlation and runtime versus batch size k for posterior MI and predictive MI."
              width="2600"
              height="1400"
              loading="lazy"
              decoding="async"
            >
            <figcaption>
              <strong>Linear-Gaussian model.</strong> Top row: correlation with the Oracle reference vs batch size $k$ for
              posterior MI (left) and predictive MI (right). Bottom row: average time per MI query vs $k$. With a fixed
              refinement budget, best-first adaptive partitioning keeps runtime essentially flat as $k$ grows while
              maintaining strong correlation.
            </figcaption>
          </figure>

          <p class="muted"><strong>Takeaway:</strong> you can hold a strict per-query budget fixed as $k$ grows without giving up strong ranking fidelity.</p>

          <figure>
            <img
              src="assets/mi_all_summary_ridge_logistic.png"
              alt="Ridge-logistic results: correlation and runtime versus batch size k for posterior MI and predictive MI."
              width="2600"
              height="1400"
              loading="lazy"
              decoding="async"
            >
            <figcaption>
              <strong>Ridge-logistic model.</strong> The same story holds in a nonconjugate setting: competitive fidelity
              without posterior samples, and predictable compute thanks to explicit global refinement budgets.
            </figcaption>
          </figure>

          <figure>
            <img
              src="assets/mi_spread_summary.png"
              alt="Posterior-MI spread summary: box and whisker plots summarizing variability across batch sizes k for correlation and runtime."
              width="2200"
              height="1600"
              loading="lazy"
              decoding="async"
            >
            <figcaption>
              <strong>Across-$k$ variability (posterior MI).</strong> Box/whisker summaries for correlation and time
              highlight stability and predictable compute for all of the batch sizes tested.
            </figcaption>
          </figure>

          <figure>
            <img
              src="assets/mi_spread_summary_predictive.png"
              alt="Predictive-MI spread summary: box and whisker plots summarizing variability across batch sizes k for correlation and runtime."
              width="2200"
              height="1600"
              loading="lazy"
              decoding="async"
            >
            <figcaption>
              <strong>Across-$k$ variability (predictive MI).</strong> The same spread summaries for the predictive-MI
              objective.
            </figcaption>
          </figure>
        </div>
      </section>

      <div class="grid">
        <section class="panel">
          <h2>Evaluation</h2>
          <ul class="bullets">
            <li>
              For each model, we estimate MI for batches of experiments of size $k$, averaged over many randomly chosen
              design subsets (apples-to-apples across methods at each $k$).
            </li>
            <li>
              <strong>Fidelity (correlation):</strong> Pearson correlation across 20 candidate batches between a method’s MI
              estimates and the Oracle MI values (higher means better MI-based ranking).
            </li>
            <li>
              Two objectives: posterior MI (learning parameters) and predictive MI (learning predictions at target
              inputs).
            </li>
            <li>
              Timing reports average time per MI evaluation (given whatever cached data the method uses), which is the
              relevant cost when selecting designs by evaluating MI many times.
            </li>
          </ul>
        </section>

        <section class="panel">
          <h2>Key takeaways</h2>
          <ul class="bullets">
            <li><strong>Predictable compute:</strong> fixed refinement budgets keep per-query runtime approximately flat as batch size $k$ grows.</li>
            <li><strong>Anytime:</strong> usable early scores that improve monotonically as you spend more budget.</li>
            <li><strong>Global refinement:</strong> spends compute on high-impact outcome regions.</li>
            <li><strong>No posterior sampling:</strong> Bayesian updating happens inside the MI estimator.</li>
            <li><strong>Unified objectives:</strong> the same machinery supports posterior and predictive MI.</li>
            <li><strong>Drop-in scorer:</strong> raw data + candidate batch in, information-gain score out.</li>
          </ul>
        </section>
      </div>

      <section class="panel">
        <h2>Estimator overview</h2>
        <p>
          Best-first adaptive partitioning is a tree-based adaptive quadrature over a discretized outcome space. It starts
          with a coarse partition, maintains an approximate posterior state per region, and repeatedly refines the
          regions with the highest estimated global impact—subject to a strict compute budget.
        </p>
        <p class="muted">
          The refinement budget is the knob: you trade runtime for fidelity, and you can keep a strict per-query latency
          target as $k$ grows.
        </p>
      </section>

      <section class="panel">
        <h2>Comparison protocol</h2>
        <p>
          Sampling-based baselines typically require posterior samples (e.g., via HMC/MCMC). That inference can be a large
          one-time fixed-data cost, but it’s still a practical barrier in real workflows. Best-first adaptive partitioning
          avoids that dependency by updating posterior state as part of MI estimation.
        </p>
        <p class="muted">
          On these benchmarks the HMC/MCMC inference step is about 0.1s (~100ms), even though the models are small. My estimator
          takes about 1ms for inference <strong>and</strong> MI estimation.
        </p>
        <p class="muted">
          To avoid subtle “training on the test set” effects, each sampling-based method uses its own independent posterior
          draw stream (not shared across methods or with the Oracle reference).
        </p>
      </section>

      <section class="panel">
        <h2>Baselines</h2>
        <details open>
          <summary>Best-first adaptive partitioning</summary>
          <p>
            Budgeted best-first refinement over outcome partitions, with approximate posterior updates attached to each
            region. Tunable compute via a global refinement budget; no posterior samples required. From an integration
            perspective, it can be exposed as a single scoring function from (model spec, observed data, candidate batch)
            to information gain.
          </p>
        </details>
        <details>
          <summary>Nested Monte Carlo (NMC)</summary>
          <p>
            General-purpose MI estimator with nested sampling loops. Broadly applicable but can be expensive and
            variance-limited unless inner sample sizes grow.
          </p>
        </details>
        <details>
          <summary>Variational nested Monte Carlo (VNMC)</summary>
          <p>
            Uses variational approximations to tighten bounds and reduce variance relative to NMC, but remains
            sample-based and still relies on inference machinery.
          </p>
        </details>
        <details>
          <summary>Variational posterior bound (Laplace)</summary>
          <p>
            Fast approximation via a bound and local posterior geometry; performance depends on how well the approximation
            matches the true posterior (often weaker in nonconjugate regimes).
          </p>
        </details>
        <details>
          <summary>Importance sampling (y-sampled)</summary>
          <p>
            Avoids exhaustive outcome enumeration by sampling outcomes with a proposal distribution; can be brittle when
            outcomes are imbalanced or proposals mismatch the predictive distribution.
          </p>
        </details>
        <details>
          <summary>Problem-specific ceiling baselines (MC analytic / exhaustive importance)</summary>
          <p>
            These are not drop-in estimators you’d deploy in a generic design loop. They are custom, model-specific
            reference computations that either exploit analytic structure or use very expensive, problem-tailored
            importance schemes (often alongside HMC) to approximate MI as accurately as practical for these benchmarks.
          </p>
          <p>
            They’re included to ground the comparisons: rather than arguing against weak baselines, the figures show how
            close each scalable method gets to a credible ceiling when custom, model-tailored tools are available.
          </p>
        </details>
      </section>

      <section class="panel callout">
        <h2>Want this in your workflow?</h2>
        <p>
          If you’re selecting experiments with MI (or need a fast, stable way to score designs without maintaining an
          MCMC pipeline), I’m happy to talk through your setup and whether a budgeted estimator is a good fit. If you can
          specify the regression model you’re using, I can deliver this as a drop-in scoring component that works directly
          on your data.
        </p>
        <div class="actions">
          <a class="button" href="mailto:joe@josephsmiller.com?subject=Mutual%20information%20design%20inquiry">Email me</a>
          <a class="text-link" href="../index.html">Back to homepage</a>
        </div>
      </section>
    </main>
  </div>
</body>
</html>
