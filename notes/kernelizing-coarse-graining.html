<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Kernalized coarse graining · Joseph S. Miller</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="How a many-to-one observation map can be rewritten as an indistinguishability kernel, turning entropy into expected surprisal of typicality and motivating similarity-sensitive entropy.">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:wght@300;400;500;600&display=swap" rel="stylesheet">
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
      },
      svg: {
        fontCache: 'global',
      },
    };
  </script>
  <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <style>
    :root {
      color-scheme: light;
      font-family: "Crimson Pro", "Times New Roman", serif;
      --ink: #1f2a37;
      --muted: #5b6673;
      --bg: #eef1f5;
      --panel: #ffffff;
      --accent: #8b3a3a;
      --accent-strong: #6d2b2b;
      --accent-soft: rgba(139, 58, 58, 0.14);
      --line: rgba(31, 42, 55, 0.12);
    }

    body {
      margin: 0;
      min-height: 100vh;
      background:
        linear-gradient(140deg, rgba(139, 58, 58, 0.05) 0%, transparent 55%),
        repeating-linear-gradient(0deg, rgba(139, 58, 58, 0.02), rgba(139, 58, 58, 0.02) 1px, transparent 1px, transparent 6px),
        var(--bg);
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 2.5rem 1.5rem;
      color: var(--ink);
    }

    .shell {
      width: min(1000px, 100%);
      background: #ffffff;
      border-radius: 26px;
      padding: 3rem;
      border: 1px solid rgba(31, 42, 55, 0.08);
      box-shadow: 0 22px 50px rgba(31, 42, 55, 0.12);
      position: relative;
      overflow: hidden;
    }

    .top-nav {
      display: flex;
      justify-content: space-between;
      align-items: center;
      gap: 1rem;
      padding-bottom: 1.25rem;
      margin-bottom: 2rem;
      border-bottom: 1px solid rgba(31, 42, 55, 0.08);
    }

    .nav-right {
      display: flex;
      gap: 1rem;
      flex-wrap: wrap;
      justify-content: flex-end;
      align-items: center;
    }

    .hero {
      display: grid;
      gap: 1.25rem;
    }

    .eyebrow {
      text-transform: uppercase;
      letter-spacing: 0.28em;
      font-size: 0.7rem;
      color: var(--accent-strong);
      font-weight: 600;
    }

    h1 {
      margin: 0;
      font-size: clamp(2.25rem, 4.2vw, 3.3rem);
      font-weight: 600;
      letter-spacing: 0.01em;
    }

    p {
      margin: 0;
      line-height: 1.7;
    }

    .lede {
      color: var(--muted);
      font-size: 1.1rem;
      max-width: 78ch;
    }

    .actions {
      display: flex;
      flex-wrap: wrap;
      gap: 0.75rem;
      align-items: center;
      margin-top: 0.25rem;
    }

    .button {
      background: var(--accent);
      color: #fff;
      padding: 0.65rem 1.3rem;
      border-radius: 999px;
      text-decoration: none;
      font-weight: 600;
      box-shadow: 0 10px 18px rgba(31, 42, 55, 0.16);
    }

    .button:hover,
    .button:focus-visible {
      background: var(--accent-strong);
      box-shadow: 0 14px 24px rgba(31, 42, 55, 0.2);
    }

    a {
      color: var(--accent-strong);
      text-decoration: none;
      font-weight: 600;
    }

    a:hover,
    a:focus-visible {
      color: var(--accent);
    }

    .text-link {
      color: var(--muted);
      font-weight: 500;
    }

    .text-link:hover,
    .text-link:focus-visible {
      color: var(--accent-strong);
    }

    .stack {
      margin-top: 2.75rem;
      display: grid;
      gap: 1.5rem;
      grid-template-columns: minmax(0, 1fr);
    }

    .grid {
      display: grid;
      grid-template-columns: repeat(2, minmax(0, 1fr));
      gap: 1.5rem;
    }

    .panel {
      background: var(--panel);
      border: 1px solid var(--line);
      border-radius: 18px;
      padding: 1.75rem;
      box-shadow: 0 12px 24px rgba(31, 42, 55, 0.08);
      position: relative;
      min-width: 0;
    }

    .panel::before {
      content: "";
      position: absolute;
      left: 1.75rem;
      top: 0.85rem;
      width: 42px;
      height: 2px;
      border-radius: 999px;
      background: var(--accent);
    }

    .panel h2 {
      margin: 0 0 1rem;
      font-size: 0.92rem;
      letter-spacing: 0.2em;
      text-transform: uppercase;
      color: var(--accent-strong);
      font-weight: 600;
    }

    .panel h3 {
      margin: 1.85rem 0 0.85rem;
      font-size: 1.35rem;
      font-weight: 600;
      letter-spacing: 0.01em;
    }

    .panel h3:first-of-type {
      margin-top: 0.5rem;
    }

    .panel p + p {
      margin-top: 0.9rem;
    }

    .panel p + .bullets {
      margin-top: 0.9rem;
    }

    .panel .bullets + p {
      margin-top: 0.9rem;
    }

    code {
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", monospace;
      font-size: 0.95em;
    }

    .bullets {
      margin: 0;
      padding-left: 1.25rem;
      line-height: 1.7;
    }

    .bullets li {
      margin: 0.35rem 0;
    }

    .math-block {
      margin: 0.5rem 0 0;
      overflow-x: auto;
      max-width: 100%;
      -webkit-overflow-scrolling: touch;
    }

    mjx-container[jax="SVG"][display="true"] {
      max-width: 100%;
      overflow-x: auto;
      overflow-y: hidden;
      -webkit-overflow-scrolling: touch;
    }

    figure {
      margin: 0;
      display: grid;
      gap: 0.85rem;
    }

    .figure-placeholder {
      border-radius: 14px;
      border: 1px dashed rgba(31, 42, 55, 0.18);
      background: rgba(31, 42, 55, 0.03);
      padding: 1.25rem 1.35rem;
      display: grid;
      gap: 0.4rem;
    }

    figcaption {
      color: var(--muted);
      line-height: 1.6;
      font-size: 1rem;
    }

    figcaption strong {
      color: var(--ink);
      font-weight: 600;
    }

    details {
      padding-top: 0.9rem;
      border-top: 1px solid rgba(31, 42, 55, 0.08);
    }

    details:first-of-type {
      padding-top: 0;
      border-top: none;
    }

    summary {
      cursor: pointer;
      font-weight: 600;
      color: var(--ink);
      list-style: none;
    }

    summary::-webkit-details-marker {
      display: none;
    }

    details p {
      margin-top: 0.65rem;
      color: var(--muted);
    }

    .callout {
      background: rgba(139, 58, 58, 0.04);
      border-color: rgba(139, 58, 58, 0.18);
    }

    .callout::before {
      background: var(--accent-strong);
    }

    .muted {
      color: var(--muted);
    }

    @media (max-width: 900px) {
      .grid {
        grid-template-columns: 1fr;
      }
    }

    @media (max-width: 700px) {
      .shell {
        padding: 2.25rem 1.75rem;
      }
    }
  </style>
</head>
<body>
  <div class="shell">
    <nav class="top-nav">
      <a class="text-link" href="../index.html">← Home</a>
      <div class="nav-right">
        <a class="text-link" href="index.html">Notes</a>
        <a class="text-link" href="mailto:joe@josephsmiller.com">Email</a>
      </div>
    </nav>

    <header class="hero">
      <div class="eyebrow">Technical note</div>
      <h1>Kernalized coarse graining</h1>
      <p class="lede">
        An algebraic reframing of shannon-style entropy motivates much of similarity-sensitive entropy: keep the uniform measure $U \sim \mathrm{Unif}[0,1]$, and interpret a many-to-one observation map by a kernel that encodes indistinguishability.
      </p>
      <ul class="bullets muted">
        <li><strong>Uniform noise + structure:</strong> represent a law as $X=\psi(U)$ with $U\sim\mathrm{Unif}[0,1]$.</li>
        <li><strong>Coarse maps → kernels:</strong> a partition map $f$ induces $K_f(u,u')=\mathbf 1\{f(u)=f(u')\}$.</li>
        <li><strong>Entropy as expected log-typicality:</strong> $H=\mathbb E[-\log \tau(U)]$ with $\tau(u)=\int K(u,u')\,du'$.</li>
      </ul>
      <div class="actions">
        <a class="button" href="mailto:joe@josephsmiller.com?subject=Kernalized%20coarse%20graining">Discuss a project</a>
      </div>
    </header>

    <main class="stack">
      <section class="panel callout">
        <h2>Takeaway</h2>
        <p>
          Any probability distribution can be generated from a uniform seed $U\sim\mathrm{Unif}[0,1]$ and a map $X=\psi(U)$. 
          If $\psi$ is invertible, then observing $X$ is equivalent to observing $U$ exactly. It is natural to conceive of $\psi$ 
          as a relabeling of each event of $U$, but in contrast to Shannon entropy, differential entropy doesn't express this semantic.
            In many other cases, what we observe or model is often a "coarsening"—some information is lost and the exact value of $U$ can't be recovered. 
          Using $Z = f(U)$ to denote this coarsening-measurement, $f$ encodes which distinctions you can make about $U$ when observing $Z$. 
          If we encode that loss of resolution explicitly, we can motivate the entirety of the similarity-sensitive entropy framework, 
          see Shannon and differential entropy as special cases and interpolate between them, while improving our notion of statistical information.
        </p>
        <p>
          The key step is to replace the many-to-one map by an indistinguishability kernel
          $K_f(u,u')=\mathbf 1\{f(u)=f(u')\}$. Its mass
          $\tau_f(u)=\int_0^1 K_f(u,u')\,du'$ is the size of the fiber containing $u$ (the preimage of $f(u)$), and Shannon entropy becomes an
          expected logarithm of typicality:
        </p>
        <div class="math-block">
          $$H(f(U))=\int_0^1 -\log \tau_f(u)\,du.$$
        </div>
        <p>
          Similarity-sensitive entropy keeps the same template but allows graded kernels $K(x,x')\in[0,1]$, making
          “resolution” explicit and letting you interpolate between infinite and finite distinguishability.
        </p>
        <p class="muted">
          For more: <a href="https://arxiv.org/abs/2601.03064" target="_blank" rel="noreferrer">Similarity-Sensitive Entropy:
          Induced Kernels and Data-Processing Inequalities (arXiv:2601.03064)</a>.
        </p>
      </section>

      <section class="panel">
        <h2>In more detail</h2>

        <h3>A universal noise source</h3>
        <p>
          The starting point is the classical “uniform representation” idea: distributions can be generated
          from a single random seed. Let
        </p>
        <div class="math-block">
          $$U \sim \mathrm{Unif}[0,1], \qquad X = \psi(U).$$
        </div>
        <p class="muted">
          In one dimension, $\psi$ is often the quantile map $F^{-1}$, so $X=F^{-1}(U)$. More generally, you can treat
          $\psi$ as a measurable map that pushes Lebesgue measure forward to whatever law you want.
        </p>
        <p>
          Any law can be read as
          <em>uniform randomness plus structure</em> (the map $\psi$).
        </p>

        <h3>Coarse observation as a partition map</h3>
        <p>
          Suppose we don’t observe $U$ precisely; we only observe which “bucket” it falls into. Formally, let
          $f:[0,1]\to\{1,\dots,m\}$ and define
          $Y := f(U)$. The fibers $A_j := f^{-1}(j)$ form a partition of $[0,1]$.
        </p>
        <div class="math-block">
          $$\mathbb P(Y=j)=\lambda(A_j)=:p_j,\qquad H(Y) = -\sum_{j=1}^m p_j \log p_j.$$
        </div>
        <details open>
          <summary>Fiber-size form of Shannon entropy</summary>
          <p>
            Because $U$ is uniform, the probability of “being indistinguishable from $u$ under $f$” is literally the
            Lebesgue measure of the fiber that contains $u$:
          </p>
          <div class="math-block">
            $$\mathbb P\!\left(f(U)=f(u)\right)=\lambda\!\left(f^{-1}(f(u))\right).$$
          </div>
          <p>
            Plugging this into the definition yields an integral identity:
          </p>
          <div class="math-block">
            $$H(Y)=\int_0^1 -\log\Big(\lambda\big(f^{-1}(f(u))\big)\Big)\,du.$$
          </div>
          <p>
            Read informally: Shannon entropy is the average surprisal of the size (mass) of the indistinguishability
            class containing the state.
          </p>
        </details>

        <h3>Kernalized coarse graining</h3>
        <p>
          Define the partition (equivalence) kernel induced by $f$:
        </p>
        <div class="math-block">
          $$K_f(u,u') := \mathbf 1\{f(u)=f(u')\}.$$
        </div>
        <p class="muted">
          Interpretation: $K_f(u,u')=1$ exactly when the observation $f$ cannot tell $u$ and $u'$ apart.
        </p>
        <p>
          From a kernel, a natural notion of “typicality” is just the kernel mass around a point:
        </p>
        <div class="math-block">
          $$\tau_f(u) := \int_0^1 K_f(u,u')\,du'.$$
        </div>
        <p>
          Since $K_f(u,u')$ is $1$ precisely on the fiber $f^{-1}(f(u))$, this reduces to
          $\tau_f(u)=\lambda(f^{-1}(f(u)))=p_{f(u)}$. Substitute into the fiber-size identity:
        </p>
        <div class="math-block">
          $$H(Y)=\int_0^1 -\log \tau_f(u)\,du.$$
        </div>
        <p class="muted">
          We don’t push the measure to $\{1,\dots,m\}$; keep the base measure on $[0,1]$ and move
          the coarse map “inside” the logarithm as a kernel.
        </p>

        <h3>From partitions to SS-entropy</h3>
        <p>
          Partition kernels are $0$–$1$. Similarity-sensitive entropy generalizes by allowing a graded similarity
          kernel $K(x,x')\in[0,1]$ (with $K(x,x)=1$) on a state space with distribution $\mu$.
        </p>
        <div class="math-block">
          $$\tau(x):=\int K(x,x')\,d\mu(x'), \qquad H_K(\mu):=\int -\log \tau(x)\,d\mu(x).$$
        </div>
        <p>
          In words, SS-entropy is expected surprisal of being typical under the similarity notion $K$. Partition kernels
          recover ordinary Shannon entropy for the corresponding coarse variable, while intermediate kernels interpolate
          between “everything is distinct” and “many things count as similar.”
        </p>
        <p class="muted">
          A benefit of this formulation is that you can define similarity where the semantics live, then transport it
          through a change of variables by pulling the kernel back.
        </p>

        <h3>Example: an even kernel on $\mathbb R$, pulled back to $[0,1]$</h3>
        <p>
          For a one-dimensional continuous distribution, the CDF gives a clean bridge between a “native” space and a
          uniform latent space. Let $X\sim\mathcal N(0,1)$ with CDF $\Phi$, and define $U:=\Phi(X)$. By the probability
          integral transform, $U\sim\mathrm{Unif}[0,1]$ (and conversely $X=\Phi^{-1}(U)$).
        </p>
        <p>
          Choose an even similarity kernel on $\mathbb R$—for example, a translation-invariant one:
        </p>
        <div class="math-block">
          $$K_\varepsilon(x,x') := \kappa\!\left(\frac{x-x'}{\varepsilon}\right), \qquad \kappa(t)=\kappa(-t), \qquad \kappa(0)=1.$$
        </div>
        <p class="muted">
          Think of $\varepsilon$ as the measurement scale in $x$-units: shrinking $\varepsilon$ demands finer resolution.
          The specific shape of $\kappa$ matters less than the fact that it encodes “closeness” in $x$.
        </p>
        <p>
          Typicality and SS-entropy on $(\mathbb R,\mu)$ (here $\mu$ is the law of $X$) are
        </p>
        <div class="math-block">
          $$\tau_\varepsilon(x):=\int K_\varepsilon(x,x')\,d\mu(x'), \qquad H_{K_\varepsilon}(\mu):=\int -\log\tau_\varepsilon(x)\,d\mu(x).$$
        </div>
        <p>
          Now move to the latent space. Pull the kernel back through the CDF/quantile maps:
        </p>
        <div class="math-block">
          $$\widetilde K_\varepsilon(u,u') := K_\varepsilon(\Phi^{-1}(u),\Phi^{-1}(u')), \qquad \widetilde\tau_\varepsilon(u):=\int_0^1 \widetilde K_\varepsilon(u,u')\,du'.$$
        </div>
        <p>
          Because $\Phi^{-1}$ pushes Lebesgue measure $\lambda$ on $[0,1]$ forward to $\mu$, this is just a change of
          variables: the entropy is unchanged, but it can be written entirely as an integral over the uniform measure:
        </p>
        <div class="math-block">
          $$H_{K_\varepsilon}(\mu)=\int_0^1 -\log \widetilde\tau_\varepsilon(u)\,du = H_{\widetilde K_\varepsilon}(\lambda).$$
        </div>
        <p class="muted">
          Nothing here is special to Gaussians of course: any continuous $F$ gives $U=F(X)\sim\mathrm{Unif}[0,1]$ and the same
          pullback identity. The Gaussian case is just a clean picture because $\Phi^{-1}$ is familiar and easy to plot.
        </p>
        <figure>
          <div class="figure-placeholder">
            <div><strong>Quantile slices: uniform bins ↔ Gaussian intervals (TODO)</strong></div>
            <div class="muted">
              Draw $u\in[0,1]$ split into equal-width bins, map them through $x=\Phi^{-1}(u)$, and shade the corresponding
              intervals under the Gaussian density.
            </div>
          </div>
          <figcaption>
            Equal-width bins in $u$ correspond to equal-probability slices of $X$:
            $$u \in [(j-1)/m, j/m] \quad \Longleftrightarrow \quad x \in [\Phi^{-1}((j-1)/m), \Phi^{-1}(j/m)].$$
            The widths in $x$ vary, but each slice has mass $1/m$.
          </figcaption>
        </figure>

        <h3>Relationship to differential entropy</h3>
        <p>
          The “infinite resolution” limit corresponds to a kernel that only matches a point to itself:
        </p>
        <div class="math-block">
          $$K_{\mathrm{id}}(x,x') := \mathbf 1\{x=x'\}, \qquad \tau(x)=\int K_{\mathrm{id}}(x,x')\,d\mu(x')=\mu(\{x\}).$$
        </div>
        <p>
          If $\mu$ is atomless (no point masses), then $\mu(\{x\})=0$ for every $x$, so $\tau(x)=0$ almost everywhere and
          $-\log \tau(x)=+\infty$. In SS-entropy terms: considering each event on a continuous space as uniquely distinct leads to
          unbounded surprisal.
        </p>
        <p>
          Differential entropy can be understood as what remains after you subtract the <em>resolution baseline</em> from a
          finite-resolution family of kernels like $K_\varepsilon$. For many “local” kernels of the form above and smooth
          densities $f$, typicality behaves like $\tau_\varepsilon(x)\approx \varepsilon f(x)$ up to a
          kernel-dependent constant, so
        </p>
        <div class="math-block">
          $$H_{K_\varepsilon}(\mu)=\mathbb E[-\log \tau_\varepsilon(X)] \approx \log(1/\varepsilon) + \mathbb E[-\log f(X)] + \text{const}.$$
        </div>
        <p class="muted">
          SS-entropy keeps the kernel (and therefore the baseline) explicit; differential entropy is the distribution part
          you get by renormalizing away that baseline as $\varepsilon\to 0$. By the pullback identity above, you can
          compute the same high-resolution behavior either on $(\mathbb R,\mu)$ or on $([0,1],\lambda)$ with the pulled-back
          kernel.
        </p>
      </section>
    </main>
  </div>
</body>
</html>
